This is a fork of Gneural Networks that has not been decommissioned.

Please contribute improvements.

I would hope to make a modular library of neural network components capable of
training and inference of modern machine learning models like those in the
news.

I'm severely mentally disabled, so this is just a bit of a beacon of hope,
rather than a plan to reach completion.

# examples

When dev left off, the system was midway through migrating from the
gneural_network/*.input format to a new nnet/*.nnet format.  The old
gneural_network tests still run.

$ autoreconf && ./configure && make -j8

$ for test in tests/*.input; do src/gneural_network "$test"; done

# nnet status

It looks like .nnet script parsing is mostly implemented, as well as fwdprop()
in src/forward.c, to run a new-style network. I don't, however, see that
fwdprop() is called anywhere. Likely it needs some debugging before everything
is plugged together.

# important considerations

I emailed bear and got this advice:

> I would leverage some "experience" from private industry and stomp on three
> tremendously important "practicalities" right from the beginning because it
> will only get harder to add them.  GN was very much a "toy" project and
> didn't have much interface with the real world. It had lots of knobs to
> twiddle and lots of parameters to get stuck in parameter hell with but it
> didn't address some practical needs of real users.
>
> First: Any of those knobs and parameters that are in the way of
> parallelization on a good GPU, needs to be diked out or at least put behind a
> big warning sign.
>
> Second: One of the most important things for any Neural-network software is
> the ability to *SAVE YOUR WORK* in the middle and pick up where you left off
> rather than starting over.  Seriously, computers crash.  You don't want a
> computer crashing to mean that the forty hours or however long it's been
> working on something to be lost.  You want to be able to pick up that forty
> hours and continue, on that machine or some other.
>
> Third: Another terribly important practicality is that meaningful AI projects
> are done on multiple machines, and you have to have ways for different
> computers working on the same project to share their work. This is actually
> far easier and lower bandwidth than you'd suppose, and it can even be done
> via the "save" mechanism I put as Item 2. But it has to be designed in. It
> could even be done meaningfully on low bandwidth internet connections.  But
> this makes robustness even more important.  When somebody drops off the
> network or quits, you don't want that to undo any work or disrupt any of the
> other machines working on it.

> You should check the associated bug database because I think I might have
> been tracking my "todo's" as bugs that needed fixing. I can't promise it
> though.  Without a "roadmap.txt" type document, which ought to be saved with
> the source code and wasn't, I'd need a couple of days looking it over to get
> the basic organization and unmet needs back into my head.

# gneural network was decommissioned?

https://web.archive.org/web/20210916155331/http://www.gnu.org/software/gneuralnetwork/

> This project has been decommissioned. This web page is kept here for historical purposes only.

https://web.archive.org/web/20211227183603/http://savannah.gnu.org/projects/gneuralnetwork

> GNU Gneural Network has been decommissioned.
