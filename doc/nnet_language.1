.\" Process this file with
.\" groff -man -Tascii nnet.1
.\"
.\" Copyright (c) 2016, Ray Dillinger <bear@sonic.net>
.\" Copyright (c) 2022, Karl Semich <0xloem@gmail.com>
.\"
.\" %%%LICENSE_START(AGPLv3+_DOC_FULL)
.\" This is free documentation; you can redistribute it and/or
.\" modify it under the terms of the GNU Affero General Public License as
.\" published by the Free Software Foundation; either version 3 of
.\" the License, or (at your option) any later version.
.\"
.\" The GNU Affero General Public License's references to "object code"
.\" and "executables" are to be interpreted as the output of any
.\" document formatting or typesetting system, including
.\" intermediate and printed output.
.\"
.\" This manual is distributed in the hope that it will be useful,
.\" but WITHOUT ANY WARRANTY; without even the implied warranty of
.\" MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
.\" GNU Affero General Public License for more details.
.\"
.\" You should have received a copy of the GNU Affero General Public
.\" License along with this manual; if not, see
.\" <http://www.gnu.org/licenses/>.
.\" %%%LICENSE_END
.\"
.\"
.TH NNET_LANGUAGE 1 "JANUARY 2017" Linux "User Manuals"
.SH NAME
nnet_language \- neural net scripting language reference manual
.SH SYNOPSIS
.B nnet
.I filename

.SH DESCRIPTION

.B nnet
is a utility for general development of neural networks.  It reads and writes files written in the nnet script
language.  See
.B nnet(1)
for instructions on using the executable.

Nnet scripts completely describe neural networks, along with such ancillary information as needed to train, test, and
deploy them.
.B nnet
reads the scripts, builds the neural networks, does whatever processing or training or deployment the script calls for,
then writes a script which describes the the network as changed by that processing.  The developer can edit nnet script
in a text editor, making changes to the network, adding a new training plan, directing it to access new sources of data,
etc, and then run
.B nnet
again on the modified script, enabling progressive staged development or midstream alterations to training parameters.

.SS Comments and whitespace:

NNet script is whitespace insensitive.  You can insert tabs, comments, line breaks, or spaces anywhere except in the
middle of tokens, without altering the meaning of the script.

A nnet script is divided into sections of several types.  These sections are:

.B  Initial Comment

.B  Nnet Configuration

.B  Node Definition

.B  Connection Definition

.B  Modification

.B  Data

.B  Training

To take them one at a time:
.SS Initial Comment
Comments may occur anywhere in a nnet script file. They begin with a \(lq#\(rq character and end with a newline.  Initial
Comments are comments at the very start of a script file.  Any non-comment character, even whitespace characters,
terminates Initial Comments. Initial Comments will be copied verbatim into the output file. Comments appearing at other
places in the script will not appear in the generated output file.  This allows project notes, including an interpreter
('crunchbang' or 'shebang') line, to be propagated through
.B nnet
to later versions of the network file.
.SS Nnet Configuration Section
Nnet Configuration Sections customize the way
.B nnet
works with this particular script.  They start with the keyword
.I StartConfig
and end with the keyword
.I EndConfig.
In between there are various statements that alter the behavior of
.B nnet
while processing that particular script.

.I Silence
commands tell
.B nnet
to turn off some of its default output.
.I Silence
may be called with multiple arguments.  Each argument must be one of the following keywords.
.PP
.I Silence(Echo)
suppresses the usual printing of the script to stdout as it is read.
.PP
.I Silence(Recurrence)
suppresses a warning about a recurrent neural network.
.PP
.I Silence(NoOutput)
supresses a warning about a network with no output nodes.
.PP
.I Silence(NoInput)
suppresses a warning about a network with no input nodes.
.PP
.I Silence(NodeInput)
and
.I Silence(NodeOutput)
suppress warnings about nodes that have no incoming or outgoing connections.
.PP
.I Silence(MultiActivation)
suppresses a warning about nodes whose activation function will be run more than once in the defined firing sequence.
This happens when processing connections in sequence means that propagating signals to and from a node will switch more
than once in the firing sequence.


.I Save
commands change the way the script output is saved.
.B nnet
normally writes
.I filename.out
once immediately after loading
.I filename.nnet
and performing any simple modifications.  Then if performing modifications that take a significant amount of time, such as training,
.B nnet
writes
.I filename.out
again when those actions are finished, overwriting the previous save.
.I Save
may be called with multiple arguments.

.I Save(\(lqString\(rq)
will change the save filename.  The file will be saved as
.I String
instead of as
.I filename.out.

.I Save(Serialize)
Is a command to insert a serial number in the name of each savefile, which avoids overwriting the earlier savefiles.
If the script being run has a serial number in its name, the serial numbers in the output file names will start from
that number.  For example if you ran
.B nnet foobar.20
to process a file named
.I foobar.20.nnet,
and there was a
.I Save(Serialize)
command in that file, then the output file will be named
.I foobar.21.out
rather than
.I foobar.20.out.
If an explicit output filename was given along with a
.I Serialize
argument, the last '.' in the savefile name will be changed into a number between two '.' characters.  For example if you ran
.B nnet foobar.20
to process a file named
.I foobar.20.nnet
and there was a command
.I Save(Serialize \(lqfoobar.nnet\(rq)
the output file would be named
.I foobar.21.nnet
rather than
.I foobar.20.out.
If a specified savefilename has no '.' character in it, then a '.' character and the serial number will be appended at
the end.

.I Save(n)
where n is an integer causes up to n periodic saves to take place during training.  Each save will take place at the end
of an epoch of training.  If n evenly divides the number of epochs then the intervals between saves will be equal.
Otherwise they'll be as close to equal as possible.  If n is less than the number of epochs of training then one save
will be made after each epoch.  If given along with a
.I Serialize
keyword argument, each save will have a new incremental serial number.

.SS  Node Definition Section
Node Definition Sections define nodes.  They start with the keyword
.I StartNodes
and end with
.I EndNodes.
In between there are
.I CreateInput, CreateHidden,
and
.I CreateOutput
statements. Each node gets an ID number when it is created.  You need to know how these ID numbers are assigned in order
to make connections.  They are assigned in ascending sequence, starting with 1, as the Node-creating statements are
processed.  But there is an exception to this sequence rule.  All input nodes have lower ID numbers than all non-input
nodes, and all output nodes have ID numbers higher than all non-output nodes.  So if a CreateInput statement is
processed after a different kind of node-creating statement, it can force the ID numbers of hidden and output nodes
which already exist to be changed to make room for the new nodes.  The same thing happens if a CreateHidden statement is
processed after a CreateOutput statement.  If the ID number of a node changes as a result of defining additional nodes,
any connections already defined using the earlier ID number are not affected.  This can be confusing because different
ID numbers then refer to the same node in different connection statements.  It's usually clearest to avoid renumbering
entirely by always defining input nodes first and output nodes last, and defining all nodes before all connections.

.I CreateInput
statements create Input nodes.  Input nodes are different from other nodes in that an input value is added its signal, and
its activation function is run, before the connection sequence is run.
.I CreateOutput
statements create Output nodes.  Output nodes are different from other nodes in that, when the connection sequence has
been run, any output nodes with nonzero signal have their activation function run.  The activation of the output nodes
is copied to the network output.
.I CreateHidden
statements create hidden nodes.  Signals arrive at nodes when connections having those nodes as destination are
processed, and are accumulated into the node's signal level using the node's accumulator function.  Whenever a
connection having a given node as source is processed, if that node has nonzero signal, its activation function is run,
establishing a new activation value for that function.  This sets the signal level back to zero.  All this applies
equally to Input and Output nodes, but in a typical feedforward network input nodes are not the destination of any
connections and output nodes are not the source of any connections.

Every node-creating statement takes three arguments; the number of nodes to create, the name of the accumulator
function, and the name of the activation function.

For example here is the Node Definition Section of the XOR network (the smallest network of conventional nodes capable
of computing XOR):


.EX
     StartNodes
          CreateInput(2 None Identity)
          CreateHidden(1 Add StepFunction)
          CreateOutput(1 Add StepFunction)
     EndNodes
.EE

.SS Connection Definition Section
Connection definition sections start with the keyword
.I StartConnections
and end with the keyword
.I EndConnections.
In between are a series of
.I Connect
statements.

.I Connect
statements have three arguments: the source node or nodes, the destination node or nodes, and the weight or weights of
each connection.  Each node may be an ID number, or an integer span of ID numbers.  The weights may be a real number, an
array of real numbers, or (when initializing) the keyword 'Randomize.'  For example here is a way to write the connection
definition statement of the XOR network:

.EX
     StartConnections
          Connect(0 {3 4} -1.0)
          Connect({1 2} {3 4} 1.0)
          Connect(3 4 -2.0)
     EndConnections
.EE

The first
.I Connect
statement defines bias connections to the hidden node (3) and the output node (4). Both connections have weight -1.  The
second connects both of the input nodes to the hidden node and the output node (4), also with weight 1.  And the last
defines a connection from the hidden node to the output node, with weight -2.

The same network can also be written this way:

.EX
     StartConnections
          Connect({0 2} {3 4} [-1.0 -1.0
                                1.0  1.0
                                1.0  1.0])
          Connect(3 4 -2.0)
     EndConnections
.EE

Here the first two statements have been combined into one, using an array of weights instead of two different shared
weights.  It connects the bias node and both the input nodes to the hidden node and the output node - and gives the six
weights as an array, with one source per line and one destination per column.

It could also be written with the whole array on one line; whitespace is not significant.

Whenever a different sequence of connection processing within a
.I Connect
statement could give different results (for example in a nonspiking recurrent network when the sources and destinations
in the same 'connect' statement overlap) this sequence of connection processing within connect statements that define
multiple connections (in ascending sequence by source) is considered canonical; the network must produce results as if
this were the actual order in which the connections were processed.

The sequence in which connections are processed is otherwise constrained by the sequence of the
.I Connect
statements defining them.

.I Connection Semantics:

A node has two values significant for connection processing: its signal and its activation.  Whenever its activation
function is run, its signal is used as an argument to its activation function to determine its activation, and the
signal is reset to zero.

In spiking networks, the activation function is run the first time in the firing sequence a node is used as the source
of a connection, and not run again during the same firing sequence.  In nonspiking networks, each time a connection is
processed, the source node is checked to see if it has nonzero signal; if so, the activation function is run and the new
activation is propagated by the connection (the product of the weight and the activation is added to the signal of the
destination node).  If the source node has zero signal, the activation function is not run, and the existing activation
is propagated by the connection.  Both signal and activation are initialized to zero when the network is started.

For purposes of this determination a node's signal is nonzero whenever connections leading to it have been processed
since the last time its activation function was run, even if the sum of all the propagated values is in fact zero.


.SS Modification Sections:
.B nnet
writes back script files as output in order to facilitate modifying networks by editing, and it's reasonable to add a
node definition or connection definition section to the end of a script, but otherwise modifying scripts by altering
definition statements directly is prone to error because it's hard to keep connections consistent when the number (and
ID numbers) of nodes are changed by the modification.  Additional utilities for network modification may be added to the
end of a script in a modification section, and nodes and connections already existing will be kept synchronized through
all changes made by the alterations.

Modification sections start with the keyword
.I StartMods
and end with
.I EndMods.
They contain
.I Delnodes, Dupenodes,
and
.I Disconnect
statements.
.B nnet
will execute the modifications immediately on load, creating an output file that describes the new network structure
directly.  Each of these instructions inflicts some degree of 'brain damage' on trained networks, but may be steps that
make further training or wider application possible or faster.

.I Delnodes
takes an ID number or ID number span as an argument and deletes the nodes having those ID numbers.  All connections to
and from those nodes are also deleted.  Other connections will not be changed, although the ID numbers of all nodes
higher-numbered than the nodes deleted will decrease.

.I Disconnect
statements take two arguments. Each argument may be an ID number or an ID number span.  Their effect is to delete all
connections from the node or nodes in the first argument to the node or nodes in the second argument.

.I Dupenodes
takes an integer and an ID number, and creates that number of near-copies of the node that has that ID number.  The new
nodes have ID numbers immediately following that of the duplicated node.  All higher numbered nodes will get increased
ID numbers.  The new nodes are connected as though all connect statements touching the original had been ID spans
including the new nodes, and they are initialized with very small randomized weights proportional to the weights of the
connections to the original node.

Input and output nodes may be deleted or duplicated the same as any other node, and this will change the number of
inputs taken or outputs produced respectively. Matching changes in the 'data' sections will to be needed.


.SS Data Sections
Data sections define data (or data sources) for training, testing, validation, and deployment.  They also define a
filename (or the name of a named pipe) to which results are to be written in deployment.  They begin with the keyword
.I StartData
and end with
.I EndData,
and in between contain
.I Data
statements.

The Syntax of a Data statement is
.EX
      Data(<data_source> <uses> <flags> <output_dest> <data>)
.EE

<data_source> starts with one of the keywords 'Immediate', 'FromFile', 'FromDirectory', or 'FromPipe'.  Any of the last
three must be followed by a string containing the filename, named pipe, or directory name to open and read from, and
also means the <data> argument may not be present.  'Immediate' means nnet should read cases inline directly from the
<data> argument.

All types of data source must give cases in the same format as the <data> argument described below. 'FromDirectory'
means that every file in that directory is to be opened and read as a data source.

<use> is one or more of the keywords 'Training', 'Testing', 'Validation', or 'Deployment', and describe what operations
this data source is to be used for.   The keywords may come in any sequence.

<flags> if present are zero or one of the keywords 'NoInput' or 'NoOutput' which serve to notify nnet that the cases it
will be reading from this source do not include one or the other, and zero or one of the keywords 'WriteNoInput'
or 'WriteNoOutput' which serve to notify nnet that Output written to a file or pipe should not include one or the other.
Flag keywords may come in any sequence.

<output_dest> may be skipped, unless 'Deployment' has been specified among the <use> arguments or 'WriteNoInput'
or 'WriteNoOutput' have been specified among the flags. If present, <output_dest> consists of either the
keyword 'ToFile' or the keyword 'ToPipe', followed by the file name or pipe name.  nnet will open the file in append
mode (existing file contents will not be overwritten) or open a named pipe, and write its output to that location as a
sequence of cases, in the same text format as the <data> segment.

<data> if present is one or more cases.  If 'ReadNoInput' or 'ReadNoOutput' are specified, each case is a single
sequence.  Otherwise, each case is a double sequence.  Single Sequences are an open square bracket, a sequence of real
numbers, and a closing square bracket.  Double Sequences are an open square bracket, a Single Sequence for Input, a
Single Sequence for output, and a closing square bracket.  The number of values in each Single Sequence must match the
network's number of inputs or number of outputs, respectively.

Here are some examples of correct Data Statements:
.EX
   Data(Immediate Training Testing Validation
       [[0.0 0.0] [0.0]]
       [[0.0 1.0] [1.0]]
       [[1.0 0.0] [1.0]]
       [[1.0 1.0] [0.0]] )
.EE
Gives the four cases of the classic XOR problem.  This same data will be used whether the script says to train, test, or
validate.  The output produced by the network will not be written anywhere, but the testing summary (accuracy) will be
written on stdout.  If the developer wanted to see the output from testing, he would write
.EX
      Data(Immediate Training Testing Validation
       [[0.0 0.0] [0.0]]
       [[0.0 1.0] [1.0]]
       [[1.0 0.0] [1.0]]
       [[1.0 1.0] [0.0]] ToPipe "stdout")
.EE
This is exactly the same as the above except that the actual output of the network will be written to stdout before the
test results are written on stderr.
.EX
   Data(FromPipe "inputstream" Deployment
        ReadNoOutput WriteNoInput ToPipe "outputstream")
.EE
Says that when Deploying, we will read input (but no output) from the pipe named inputstream and write output(but not
input) to the pipe named outputstream.
.EX
   Data(FromFile "Examples.nnx" Testing
        ToFile "Results.nnx")
.EE

Directs nnet, when testing, to read complete cases (input and output) from Examples.nnx, then write complete cases to
Results.nnx.  The results given in Examples.nnx will be compared to the results produced by the network for accuracy;
the results produced by the network will be written in cases with the inputs that produced them.  Summary testing
results and accuracy will be displayed on stdout.
.EX
  Data(FromDirectory "ExampleDir" Testing)
.EE
Directs nnet, when testing, to open every file in ExampleDir read cases from it, and not to bother writing the output
anywhere.  This statement just allows the developer to view the testing summary and accuracy achieved which will be
written on stdout.

.SS Plan Section

Plan sections start with the keyword "StartPlan" and end with the keyword "EndPlan".  nnet takes these plans as
instructions to carry out.  Any script files saved (due to save serialization, etc) while the plans are in progress will
contain plan statements needed to complete any plans not yet carried out.  When all plans have been carried out, nnet
will exit, saving a script with no Plan Section.

Between the keywords StartPlan and EndPlan there may be zero or more Plan statements.  Plan statements are of four
types: TestingPlan, TrainingPlan, ValidationPlan, and DeploymentPlan.  By default, training, testing, or validation
plans will produce a report on stdout, and deployment plans will not.  The report may be redirected to a different
output, or a report may be called for with a deployment plan, using 'ReportTo' followed by the name of a file or pipe to
write to.

The arguments of TrainingPlan must start with the name of a training method.  Additional arguments to TrainingPlan
depend on the training method.  All training methods support TrainingGoal, BatchSize, EpochSize, MaxEpoch, and MinEpoch
arguments.  TrainingGoal must be followed by a real number (an accuracy considered good enough to stop training).
BatchSize, EpochSize, MaxEpoch, and MinEpoch must be followed by Integers.

BatchSize is the number of examples to process between training updates.  EpochSize is the number of batches to process
per epoch.  Between Epochs, accuracy is measured, report output is generated, and a check to see whether training is
finished is made.  Also between epochs, depending on save serialization, an updated output script may be saved.  Other
actions may happen between epochs depending on the training method and/or parameters selected.

MaxEpoch and MinEpoch are stopping conditions.  If the count of epochs is less than MinEpoch, then training will
continue even if the TrainingGoal accuracy has been reached.  If the count of epochs is greater than MaxEpoch,
training will be stopped even if TrainingGoal accuracy has not been reached.

Additional arguments that are meaningful with GradientDescent training plans include 'LearningRate.'  This is the
size of the weight adjustments to make after each batch when training.

In the absence of specific arguments assigning values to these parameters, Gradient Descent training plans default to a
training rate of 0.01, an accuracy goal of 0.95, a batch size of 1, an epoch size of 10, and a MaxEpoch of 1000.  These
are suitable values for a reasonable class of small problems, but will often need updating depending on what problem
you're working on.


.B SECTION INCOMPLETE.  TBD


.SH AUTHOR
Man page written by Ray Dillinger <bear at sonic dot net>
.SH COPYRIGHT
.B nnet
is Copyright (C) 2016-2017 Ray Dillinger, Jean-Michel Sellier, and the Gneural_Network project.
Permission is granted to modify and distribute the software and its sources subject to the conditions of the GNU Public
License, version 3 or later.  Permission is granted to distribute the documentation, including this man page, subject to
the conditions of the GNU Free Documentation License, version 1.3 or later.


.SH REPORTING BUGS
.B nnet
is part of the gneural_network project. You may report or review bugs at
.UR gneural_network project bug database
https://savannah.gnu.org/bugs/?group=gneuralnetwork
.UE

.SH See Also
.B nnet(1)
